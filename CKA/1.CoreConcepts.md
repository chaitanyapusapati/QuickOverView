# **Kubernetes Core Concepts**

**Container/Cluster/Pod/Node** : https://medium.com/faun/understanding-nodes-pods-containers-and-clusters-778dbd56ade8

## **KUBERNETES** :- 
The purpose of Kubernetes is to host your applications in the form of containers in an automated fashion
so that you can easily deploy as many instances of your application as required and easily enable communication
between different services within your application.
			  
The Kubernetes cluster consists of a set of nodes which may be physical or virtual, on-premise or
on cloud that host applications in the form of containers.


## **MASTER NODE** : 
The Master node is responsible for managing the Kubernetes cluster. Storing information regarding the different nodes, planning which containers goes where, monitoring the nodes and containers on them etc. 

The Master node does all of these using a set of components together known as the control plane components.


### **ETCD Cluster :** 
It stores info about cluster. Etcd is a database that stores information in a key-value format.
When you run ETCD it starts a service that listens on Port 2379 by default. The ETCD datastore stores information regarding the cluster such as the nodes, pods, configs, secrets, accounts, roles, bindings and others.

Every information you see when you run the kubectl get command is from the ETCD server. Every change you make to your cluster, such as adding additional nodes or deploying pods or replica sets are updated in the ETCD server. **Only once it is updated in the ETCD server, is the change considered to be complete.**
					   
 
It happens to be on the IP of the server and on port 2379, which is the default port on which etcd listens. 
This is the URL that should be configured on the kube-api server when it tries to reach the etcd server.
If you setup your cluster using kubeadm then kubeadm deploys the ETCD server for you as a POD in the kube-system namespace. 


###	**Kube API :** 
It is Responsible for Orchestration of services.
The kube-apiserver is at the center of all the different tasks that needs to be performed to make a change in the cluster.

To summarize, the kube-api server is responsible for Authenticating and validating requests, retrieving and updating data in ETCD data store, in fact, kube-api server is the only component that interacts directly with the etcd datastore.The other components such as the scheduler, kube-controller-manager & kubelet uses the API server to perform updates in the cluster in their respective areas.


###	**kube Scheduler :** 
Responsible for Scheduling applications or containers or nodes.<br>
A scheduler identifies the right node to place a container based on the containers, Resource requirements, the worker nodes capacity or any other policies or constraints such as tents and tolerations or node affinity rules that are on them.


###	**Controller Manager :**
controllers take care of diffrent functions. A controller is a process that continuously monitors the state of various components within the system and works towards bringing the whole system to the desired functioning state.

For example the *Node controller* is responsible for monitoring the status of the nodes and taking necessary
actions to keep the application running. It does that through the kube-api server. The node controller checks the status of the nodes every 5 seconds. That way the node controller can monitor the health of the nodes, if it stops receiving heartbeat from a node, the node is marked as unreachable but it waits for 40 seconds before marking it unreachable and after a node is marked unreachable it gives it five minutes to come back up if it doesnâ€™t, it removes the PODs assigned to that node and provisions them on the healthy ones and if the PODs are part of a replica set then its the job of replication controller.<br>
*Replication controller* is responsible for monitoring the status of replica sets and ensuring that the desired number of PODs are available at all times within the set. If a pod dies it creates another one. <br>

Now those were just two examples of controllers.There are many more such controllers available within kubernetes. Now how do you see these controllers and where are they located in your cluster. They're all packaged into a single process known as kubernetes controller manager. When you install the kubernetes controller manager the different controllers get installed as well.

	
## WORKER NODES :
###	**Kubelet :** 
Listens instructions from kubeAPI server and manages containers.<br>
A kubelet is an agent that runs on each node in a cluster. It listens for instructions from the kube-api server and deploys or destroys containers on the nodes as required. The kube-api server periodically fetches status reports from the kubelet to monitor the state of nodes and containers on them.
					
The kubelet in the kubernetes worker node, registers the node with the kubernetes cluster. When it receives instructions to load a container or a POD on the node, it requests the container run time engine, which may be Docker, to pull the required image and run an instance. The kubelet then continues to monitor the state of the POD and the containers in it and reports to the kube-api server on a timely basis.
					
If you use kubeadm tool to deploy your cluster, it does not automatically deploy the kubelet. Now that's the difference from the other components. You must always manually install the kubelet on your worker nodes. Download the installer, extract it and run it as a service.
					

###	**Kube-Proxy :** 
Communication between worker nodes are enabled by component that runs on the worker node known as the Kube-proxy service. 
The Kube-proxy service ensures that the necessary rules are in place on the worker nodes to allow the containers running on them to reach each other.


## **Basic Commands :**
**To create pods :**
```
$ kubectl run nginx --image nginx
```
-> It first creates a POD automatically and deploys an instance of the image downloaded using runtime engine.<br>
-> In this case the nginx image, downloaded from the docker hub repository.

**To Delete a Pod**
```
$ kubectl delete <resource_name> <object_name>
$ kubectl delete pod <pod_name>
```
**list of PODs :**   
```
$ kubectl get pods
```	
**Long list of PODs :**  
```
$ kubectl get pods -o wide
```	
**All objects :**  
```
$ kubectl get all 
```
-> To see all object create at once.

**POD Details :**  
```
$ kubectl describe pod <podname> 
```
**Resource Details :**  
```
$ kubectl explain <resourceName> 
```
-> To view the details about a resource type

**Create a resource using yaml file :**   
```
$ kubectl create -f <fileName>.yml
```	
**Apply command :**   
```
$ kubectl apply -f <fileName>
```	
-> To apply changes when we edit an object file, it automatically checks the changes and applies it to the pods if needed by deleting and re-creating pods.<br>
-> Checks whether a object with that file exists and if not, creates a new object.

**Edit Command :**   
```
$ kubectl edit <resourceName> <objectName>
$ kubectl edit pod <podName>
$ kubectl edit deployment <deploymentName>
```	
-> To edit a object definition file and automatically apply changes to the object.<br>
-> **Note :** There are restrictions on what are allowed to change in definition since the object is running but, a `temp` file is created and stored with the changes that are not allowed to apply unless we delete and re-create the object.

**Alias :**  
```
 $ alias <shortcut>='<Actual_Command>'
 $ alias k='kubectl' 
```
-> To create shortcuts of commands

**Creating a Yaml file :**   
```
$ kubectl run <name> --image=<imageName> --dry-run=client -o yaml > <filename>.yml
```	
-> Creates a yaml file but doesn't launch a object. You have to launch it using *Create* or *Apply* command.
<br>
 <br>

**Pod definition file :**
```yaml
apiversion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
specs:
  containers:
  - name: nginx
    image: nginx
```

**Replication controller definition file :**
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: front-end
spec:
 template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
     containers:
     - name: nginx-container
       image: nginx
 replicas: 3
```
**To Create the replication controller :**
 ```
 $ kubectl create -f rc-definition.yaml
 ```
 **To list all the replication controllers :**
 ```
 $ kubectl get replicationcontroller
 ```

## ReplicaSets ensures that desired number of PODs always run
**Replication Set definition file :**
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: front-end
spec:
 template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
     containers:
     - name: nginx-container
       image: nginx
 replicas: 3
 selector:
   matchLabels:
    type: front-end
```
**To get Replicasets :**
```
$ kubectl get replicasets
```
**Increasing Replicas :**  
```
$ kubectl replace -f <filename>.yml
```	
-> Increase by updating the replicas in definition file.

**Increasing Replicas using `scale` command :**   
```
$ kubectl scale --replicas=6 replicaset <Replicaset Name>
```
```
$ kubectl scale --replicas=6 -f <filename>.yml
```	
-> **Note :** Using *scale* command with filename does increase replicas but doesn't increase the set no in definition file.

<br>

## **Deployments :**
Say for example you have a web server that needs to be deployed in a production environment. You need not one but many such instances of the web server running for obvious reasons. Secondly whenever newer versions of application builds become available on the docker registry you would like to upgrade your Docker instances seamlessly. However when you upgrade your instances you do not want to upgrade all of them at once as we just did. This may impact users accessing our applications, so you might want to upgrade them one after the other. And that kind of upgrade is known as rolling updates. Suppose one of the upgrades you performed resulted in an unexpected error and you're asked to undo the recent change you would like to be able to roll back the changes that were recently carried out. Finally, say for example you would like to make multiple changes to your environment such as upgrading the underlying WebServer versions, as well as scaling your environment and also modifying the resource allocations etc.
WebServer versions, as well as scaling your environment and also modifying the resource allocations etc. You do not want to apply each change immediately after the command is run instead you would like to apply a pause to your environment, make the changes and then resume so that all changes are rolled-out together. All of these capabilities are available with the kubernetes Deployments.

The deployment provides us with the capability to upgrade the underlying instances seamlessly using rolling updates, undo changes, and pause and resume changes as required.

Once the file is ready and when you run the `kubectl create command` and specify the deployment definition file. Then run the `kubectl get deployments` command to see the newly created deployment. `The deployment automatically creates a replica set.`
So if you run the `kubectl get replcaset command`,  You will be able to see a new replica set in the name of the deployment.
The replicasets ultimately create pods, so if you run the `kubectl get pods` command you will be able to see the pods with the name of the deployment and the replicaset.

**Deployments definition file :**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
 template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
     containers:
     - name: nginx-container
       image: nginx
 replicas: 3
 selector:
   matchLabels:
    type: front-end
```

**View deployments :**
```
$ kubectl get deployments
```
**Change the image of runnnig deployment**
```
$ kubectl set image deployment nginx nginx=nginx:1.18
```
**Expose deployment :**
```
$ kubectl expose deployment nginx --port 80
```
<br>

## **NameSpaces :**  
**Create NameSpace :**
```
$ kubectl create namespace <nameSpaceName>
$ kubectl create -f <nameSpace Definition file>
```
**NameSpace definition file**
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: dev
```
**Create Object in a particular NameSpace :**
```
$ kubectl create -f <filename> --namespace=<nameSpaceName>
```	
**Change in to particular NameSpace :**
```
$ kubectl config set-context $(kubectl config current-context) --namespace=<nameSpaceName>
```	
**view the current NameSpace :**
```
$ kubectl config current-context
```	
**View Pods in all NameSpaces :**
```
$ kubectl get pods --all-namespaces
```	
**Get Pods in particular Namespace :**
```
$ kubectl get pods --namespace=<namespace_name>
$ kubectl get pods -n=<namespace_name>
```
**Connect to database in own NameSpace :**
```
$ kubectl connect <db-ServiceName>
$ kubectl connect db-service
```
**Connect to database in diffrent NameSpace :**
```
$ kubectl connect db-service.<nameSpaceName>.svc.cluster.local
```

## **Resource Quota :**
 Every Node has set of resources which are by used by pods that run on that Node. You can set resource quota by creating resource quota definition file.

 **Resource Quota definition file**
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi
```
<br>

## **Services :** 
Services enable communication between various components within and outside of the application.<br> 
Kubernetes services helps us connect applications together with other applications or users. The kubernetes service is an object just like PODs, Replica Sets and Deployments. <br>
There are various Services available.

**The First one is `NodePort` :**
NodePortService listens to a port on the Node and forward requests on that port, to a port on the POD running the web application. This type of service is known as `NodePortservice` because the service listens to a port on the Node and forwards requests to PODs. 
NodePorts can only be in a valid range which by default is from 30000 to 32767.

**The second is cluster IP :**
The service creates a virtual IP inside the cluster to enable communication between different services such as a set of front end servers to a set of back end servers.

**The third type is a LoadBalancer :** 
It provisions a load balancer for our service in supported cloud providers. This Service is used to distribute load across the different web servers in your front end tier.

**NOTE :** If you dont specify the type by default it takes ClusterIP.
<br>

**NodePort service definition file :**
```yaml
apiVersion: v1
kind: Service
metadata:
 name: myapp-service
spec:
 types: NodePort
 ports:
 - targetPort: 80
   port: 80
   nodePort: 30008
 selector:
   app: myapp
   type: front-end
```
**Cluster IP Service definition file :**
```yaml
apiVersion: v1
kind: Service
metadata:
 name: back-end
spec:
 types: ClusterIP
 ports:
 - targetPort: 80
   port: 80
 selector:
   app: myapp
   type: back-end
```
**View all services running in the system :**
```
$ kubectl get services
``` 
**To expose a port for a service**
```
$ kubectl expose pod <pod_name> --port=<port_no>
```
**To create a pod and a service exposing a port**
```
$ kubectl run <pod_name> --image=<image-name> --port=<port_name> --expose
```
**Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379 :**
```
$ kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
```
-> This will automatically use the pod's labels as selectors
```
$ kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml
```
-> This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service.
**Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes :**
```
$ kubectl expose pod nginx --port=80 --name nginx-service --type=NodePort --dry-run=client -o yaml
```
-> This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod
```
$ kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
```
-> This will not use the pods labels as selectors.